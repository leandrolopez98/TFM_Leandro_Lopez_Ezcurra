{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt: quiero copiar la carpeta de github leandrolopez98/TFM-Master4-0\n",
    "\n",
    "!git clone https://github.com/leandrolopez98/TFM-Master4-0.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import torch\n",
    "import copy\n",
    "import random\n",
    "\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torch import nn\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import albumentations as A\n",
    "from torchvision import models\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "torch.cuda.empty_cache()\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dict = {'detergentes': 0,\n",
    " 'carton': 1,\n",
    " 'latas': 2,\n",
    " 'frascos': 3,\n",
    " 'botella de plastico': 4,\n",
    " 'vidrio': 5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_n_dict(root,train_ratio = None):\n",
    "    path_im = []\n",
    "    label_im = [] \n",
    "    for k in os.listdir(root):\n",
    "        big_class = os.path.join(root,k)\n",
    "        for i in os.listdir(big_class):\n",
    "            per_class_path = os.path.join(big_class,i)\n",
    "            for j in os.listdir(per_class_path):\n",
    "                path_im.append(os.path.join(per_class_path,j)) \n",
    "                label_im.append(class_dict[i])\n",
    "    if train_ratio :\n",
    "        return train_test_split(path_im,label_im,train_size=train_ratio,random_state=42)\n",
    "    return (path_im,label_im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self,path,label,transforms = None):\n",
    "        self.path_image = path\n",
    "        self.labels = label\n",
    "        self.transforms = transforms\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self,idx):\n",
    "        image = cv2.imread(self.path_image[idx])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        label = self.labels[idx]\n",
    "        if self.transforms:\n",
    "            image = self.transforms(image = image)\n",
    "        return image,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustes de Data Augmentation \n",
    "data_transforms = {\n",
    "    'train': A.Compose([\n",
    "        A.LongestMaxSize(max_size=512, interpolation=2),  # Ajusta el tamaño máximo a 512\n",
    "        A.PadIfNeeded(min_height=512, min_width=512),  # Ajusta a 512x512\n",
    "        A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=45, p=0.8),  # Reduce rotate_limit\n",
    "        A.RGBShift(r_shift_limit=15, g_shift_limit=15, b_shift_limit=15, p=0.5),\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.5),\n",
    "        A.HorizontalFlip(),\n",
    "        A.VerticalFlip(),\n",
    "        A.CoarseDropout(max_holes=20, max_height=16, max_width=16, p=0.5),  # Reduce CoarseDropout\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "    ]),\n",
    "    'val': A.Compose([\n",
    "        A.LongestMaxSize(max_size=512, interpolation=2),\n",
    "        A.PadIfNeeded(min_height=512, min_width=512),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Cargar datasets y DataLoader\n",
    "train_path, val_path, train_label, val_label = path_n_dict('train_crops', train_ratio=0.8)\n",
    "test_path, test_label = path_n_dict('test_crops')\n",
    "\n",
    "train_dataset = MyDataset(train_path, train_label, transforms=data_transforms[\"train\"])\n",
    "val_dataset = MyDataset(val_path, val_label, transforms=data_transforms[\"val\"])\n",
    "test_dataset = MyDataset(test_path, test_label, transforms=data_transforms[\"val\"])\n",
    "\n",
    "# Ajusta batch_size y num_workers según tu hardware\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(inp, title=None):\n",
    "    \"\"\"Display image for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    plt.figure(figsize=(16, 6))\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)\n",
    "\n",
    "key_list = list(class_dict.keys())\n",
    "inputs,classes = next(iter(train_loader))\n",
    "out = torchvision.utils.make_grid(inputs['image'])\n",
    "\n",
    "imshow(out, title=[key_list[x.item()] for x in classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar modelo EfficientNet-B5 preentrenado\n",
    "base_model = models.efficientnet_b5(weights='IMAGENET1K_V1')\n",
    "base_model.classifier = nn.Sequential(\n",
    "    nn.BatchNorm1d(2048),\n",
    "    nn.Linear(2048, 512),\n",
    "    nn.LeakyReLU(0.1),\n",
    "    nn.BatchNorm1d(512),\n",
    "    nn.Dropout(0.4),  # Ajustar Dropout para regularización adicional\n",
    "    nn.Linear(512, 256),\n",
    "    nn.LeakyReLU(0.1),\n",
    "    nn.BatchNorm1d(256),  # Añadir BatchNorm adicional\n",
    "    nn.Dropout(0.3),  # Ajustar Dropout nuevamente\n",
    "    nn.Linear(256, 128),  # Añadir capa adicional\n",
    "    nn.LeakyReLU(0.1),\n",
    "    nn.Linear(128, 6)  # Salida final con 6 clases\n",
    ")\n",
    "base_model = nn.DataParallel(base_model)\n",
    "base_model.to(device)\n",
    "\n",
    "# Ajuste de optimizador y scheduler\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(base_model.parameters(), lr=0.001, weight_decay=0.01)  # Ajustar weight_decay\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=1, verbose=True)  # Ajuste más agresivo\n",
    "\n",
    "# Implementación de Early Stopping\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, delta=0.0001):  # Incrementar paciencia y ajustar delta\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.best_score = None\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f\"EarlyStopping counter: {self.counter} out of {self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.counter = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "impact_matrix = {\n",
    "    # Detergentes mal clasificados\n",
    "    (0, 1): 12,  # Detergentes clasificados como cartón: posible contaminación del papel reciclado con químicos.\n",
    "    (0, 2): 28,  # Detergentes clasificados como latas: riesgo de contaminación química en metales reciclados.\n",
    "    (0, 3): 18,  # Detergentes clasificados como frascos: residuos químicos pueden mezclarse con vidrio reciclado.\n",
    "    (0, 4): 35,  # Detergentes clasificados como botella de plástico: alto riesgo de contaminación química.\n",
    "    (0, 5): 30,  # Detergentes clasificados como vidrio: problemas en el reciclaje del vidrio con residuos peligrosos.\n",
    "\n",
    "    # Cartón mal clasificado\n",
    "    (1, 0): 8,   # Cartón clasificado como detergentes: menores costos pero puede dañar procesos de reciclaje.\n",
    "    (1, 2): 15,  # Cartón clasificado como latas: puede causar contaminación de procesos de reciclaje de metal.\n",
    "    (1, 3): 10,  # Cartón clasificado como frascos: afecta la calidad del reciclado de vidrio.\n",
    "    (1, 4): 20,  # Cartón clasificado como botella de plástico: problemas en el reciclaje de plásticos.\n",
    "    (1, 5): 22,  # Cartón clasificado como vidrio: contaminación cruzada con residuos orgánicos.\n",
    "\n",
    "    # Latas mal clasificadas\n",
    "    (2, 0): 35,  # Latas clasificadas como detergentes: contaminación severa por residuos metálicos y químicos.\n",
    "    (2, 1): 20,  # Latas clasificadas como cartón: puede causar daños al proceso de reciclaje de papel.\n",
    "    (2, 3): 25,  # Latas clasificadas como frascos: mezclas indeseables en vidrio reciclado.\n",
    "    (2, 4): 30,  # Latas clasificadas como botella de plástico: contamina plásticos reciclados con residuos metálicos.\n",
    "    (2, 5): 27,  # Latas clasificadas como vidrio: contamina la cadena de reciclaje del vidrio.\n",
    "\n",
    "    # Frascos mal clasificados\n",
    "    (3, 0): 22,  # Frascos clasificados como detergentes: contaminación química que afecta procesos de reciclaje.\n",
    "    (3, 1): 18,  # Frascos clasificados como cartón: contaminación y menor calidad del papel reciclado.\n",
    "    (3, 2): 20,  # Frascos clasificados como latas: mezclas indeseables en metales reciclados.\n",
    "    (3, 4): 24,  # Frascos clasificados como botella de plástico: problemas en procesos de reciclaje de plástico.\n",
    "    (3, 5): 12,  # Frascos clasificados como vidrio: impacto menor pero aún presente.\n",
    "\n",
    "    # Botella de plástico mal clasificada\n",
    "    (4, 0): 40,  # Botella de plástico clasificada como detergentes: alta contaminación química.\n",
    "    (4, 1): 25,  # Botella de plástico clasificada como cartón: contamina el papel reciclado.\n",
    "    (4, 2): 32,  # Botella de plástico clasificada como latas: residuos plásticos afectan la calidad de los metales.\n",
    "    (4, 3): 38,  # Botella de plástico clasificada como frascos: contaminantes plásticos en vidrio.\n",
    "    (4, 5): 35,  # Botella de plástico clasificada como vidrio: afecta el reciclaje del vidrio.\n",
    "\n",
    "    # Vidrio mal clasificado\n",
    "    (5, 0): 30,  # Vidrio clasificado como detergentes: puede causar daños en el reciclaje químico.\n",
    "    (5, 1): 20,  # Vidrio clasificado como cartón: afecta la calidad del reciclado de papel.\n",
    "    (5, 2): 25,  # Vidrio clasificado como latas: puede dañar procesos de reciclaje de metales.\n",
    "    (5, 3): 18,  # Vidrio clasificado como frascos: menor impacto pero puede mezclar residuos.\n",
    "    (5, 4): 22,  # Vidrio clasificado como botella de plástico: afecta la calidad del reciclado de plásticos.\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustar Early Stopping para evitar sobreajuste\n",
    "early_stopping = EarlyStopping(patience=10, delta=0.0005)  # Ajuste de patience y delta\n",
    "\n",
    "# Modificación del ciclo de entrenamiento y validación para evaluar el impacto ambiental\n",
    "def train(model, criterion, optimizer, scheduler, epochs, resume_train=False, PATH=None):\n",
    "    if resume_train:\n",
    "        checkpoint = torch.load(PATH)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    start = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_epoch = None\n",
    "    best_acc = 0.0\n",
    "    best_loss = float('inf')\n",
    "    t_list_loss = []\n",
    "    t_list_acc = []\n",
    "    v_list_loss = []\n",
    "    v_list_acc = []\n",
    "\n",
    "    # Listas para guardar las métricas por época\n",
    "    epoch_precision = []\n",
    "    epoch_recall = []\n",
    "    epoch_f1 = []\n",
    "    epoch_accuracy = []\n",
    "    epoch_impact = []  # Nueva lista para almacenar el impacto ambiental total por época\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_start = time.time()\n",
    "        print(f\"Epoch: {epoch + 1}/{epochs}\")\n",
    "\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_acc = 0.0\n",
    "        val_loss = 0.0\n",
    "        val_acc = 0.0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs['image'].to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            train_acc += torch.sum(preds == labels)\n",
    "\n",
    "        # Almacenar etiquetas y predicciones de validación\n",
    "        val_true_labels = []\n",
    "        val_preds = []\n",
    "\n",
    "        total_impact = 0  # Inicializar el impacto total para la época\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs = inputs['image'].to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                preds = outputs.argmax(dim=1)\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                val_acc += torch.sum(preds == labels)\n",
    "\n",
    "                val_true_labels.extend(labels.cpu().numpy())\n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "                # Calcular el impacto ambiental de los errores\n",
    "                for true_label, pred_label in zip(labels.cpu().numpy(), preds.cpu().numpy()):\n",
    "                    if true_label != pred_label:\n",
    "                        total_impact += impact_matrix.get((true_label, pred_label), 0)\n",
    "\n",
    "        # Convertir listas a arrays de NumPy\n",
    "        val_true_labels = np.array(val_true_labels)\n",
    "        val_preds = np.array(val_preds)\n",
    "\n",
    "        # Calcular métricas por época\n",
    "        val_precision = precision_score(val_true_labels, val_preds, average='weighted')\n",
    "        val_recall = recall_score(val_true_labels, val_preds, average='weighted')\n",
    "        val_f1 = f1_score(val_true_labels, val_preds, average='weighted')\n",
    "        val_accuracy = accuracy_score(val_true_labels, val_preds)\n",
    "\n",
    "        # Guardar métricas por época\n",
    "        epoch_precision.append(val_precision)\n",
    "        epoch_recall.append(val_recall)\n",
    "        epoch_f1.append(val_f1)\n",
    "        epoch_accuracy.append(val_accuracy)\n",
    "        epoch_impact.append(total_impact)  # Guardar el impacto total de la época\n",
    "\n",
    "        train_loss = train_loss / len(train_dataset)\n",
    "        train_acc = train_acc.double() / len(train_dataset)\n",
    "\n",
    "        val_loss = val_loss / len(val_dataset)\n",
    "        val_acc = val_acc.double() / len(val_dataset)\n",
    "\n",
    "        t_list_loss.append(train_loss)\n",
    "        t_list_acc.append(train_acc.item())\n",
    "        v_list_loss.append(val_loss)\n",
    "        v_list_acc.append(val_acc.item())\n",
    "\n",
    "        print(f'Train Loss: {train_loss:.4f} Acc: {train_acc:.4f}')\n",
    "        print(f'Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}')\n",
    "        print(f'Precision: {val_precision:.4f} Recall: {val_recall:.4f} F1 Score: {val_f1:.4f} Accuracy: {val_accuracy:.4f}')\n",
    "        print(f'Impacto Ambiental Total: {total_impact}')  # Mostrar el impacto total de la época\n",
    "\n",
    "        # Ajuste de scheduler\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        # Implementación de Early Stopping\n",
    "        early_stopping(val_loss)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping activated\")\n",
    "            break\n",
    "\n",
    "        # Guardar mejor modelo\n",
    "        if val_acc > best_acc or (val_acc == best_acc and val_loss < best_loss):\n",
    "            best_acc = val_acc\n",
    "            best_epoch = epoch + 1\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            best_loss = val_loss\n",
    "\n",
    "    total_time = time.time() - start\n",
    "    print(f'Training complete in {total_time // 60:.0f}m {total_time % 60:.0f}s')\n",
    "    print(f'Best val Epoch: {best_epoch}')\n",
    "    print(f'Val Acc: {best_acc:.4f}')\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    print(\"Model:\", model)\n",
    "\n",
    "    # Retornar las métricas junto con las listas de pérdidas, accuracies y el impacto ambiental\n",
    "    return model, t_list_loss, t_list_acc, v_list_loss, v_list_acc, epoch_precision, epoch_recall, epoch_f1, epoch_accuracy, epoch_impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Número de épocas\n",
    "epochs = 25\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "model_trained, t_list_loss, t_list_acc, v_list_loss, v_list_acc, epoch_precision, epoch_recall, epoch_f1, epoch_accuracy, epoch_impact = train(\n",
    "    base_model,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    epochs,\n",
    "    resume_train=False,\n",
    "    PATH='model_b5.pt'  # Cambia esta ruta según tu necesidad\n",
    ")\n",
    "\n",
    "# Guardar el modelo y el estado del optimizador, incluyendo las métricas adicionales\n",
    "torch.save({\n",
    "    'model_state_dict': model_trained.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    't_list_loss': t_list_loss,\n",
    "    't_list_acc': t_list_acc,\n",
    "    'v_list_loss': v_list_loss,\n",
    "    'v_list_acc': v_list_acc,\n",
    "    'epoch_precision': epoch_precision,\n",
    "    'epoch_recall': epoch_recall,\n",
    "    'epoch_f1': epoch_f1,\n",
    "    'epoch_accuracy': epoch_accuracy,\n",
    "    'epoch_impact': epoch_impact,  # Guardar el impacto ambiental\n",
    "}, 'model_b5_1.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustar Early Stopping para evitar sobreajuste\n",
    "early_stopping = EarlyStopping(patience=10, delta=0.0005)  # Ajuste de patience y delta\n",
    "\n",
    "# Modificación del ciclo de entrenamiento\n",
    "def train(model, criterion, optimizer, scheduler, epochs, resume_train=False, PATH=None):\n",
    "    if resume_train:\n",
    "        checkpoint = torch.load(PATH)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    start = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_epoch = None\n",
    "    best_acc = 0.0\n",
    "    best_loss = float('inf')  # Inicializar mejor pérdida\n",
    "    t_list_loss = []\n",
    "    t_list_acc = []\n",
    "    v_list_loss = []\n",
    "    v_list_acc = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_start = time.time()\n",
    "        print(f\"Epoch: {epoch + 1}/{epochs}\")\n",
    "\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_acc = 0.0\n",
    "        val_loss = 0.0\n",
    "        val_acc = 0.0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs['image'].to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            train_acc += torch.sum(preds == labels)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs = inputs['image'].to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                preds = outputs.argmax(dim=1)\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                val_acc += torch.sum(preds == labels)\n",
    "\n",
    "        train_loss = train_loss / len(train_dataset)\n",
    "        train_acc = train_acc.double() / len(train_dataset)\n",
    "\n",
    "        val_loss = val_loss / len(val_dataset)\n",
    "        val_acc = val_acc.double() / len(val_dataset)\n",
    "\n",
    "        t_list_loss.append(train_loss)\n",
    "        t_list_acc.append(train_acc.item())\n",
    "        v_list_loss.append(val_loss)\n",
    "        v_list_acc.append(val_acc.item())\n",
    "\n",
    "        print(f'Train Loss: {train_loss:.4f} Acc: {train_acc:.4f}')\n",
    "        print(f'Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}')\n",
    "\n",
    "        # Ajuste de scheduler\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        # Implementación de Early Stopping\n",
    "        early_stopping(val_loss)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping activated\")\n",
    "            break\n",
    "\n",
    "        # Guardar mejor modelo\n",
    "        if val_acc > best_acc or (val_acc == best_acc and val_loss < best_loss):\n",
    "            best_acc = val_acc\n",
    "            best_epoch = epoch + 1\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            best_loss = val_loss\n",
    "\n",
    "    total_time = time.time() - start\n",
    "    print(f'Training complete in {total_time // 60:.0f}m {total_time % 60:.0f}s')\n",
    "    print(f'Best val Epoch: {best_epoch}')\n",
    "    print(f'Val Acc: {best_acc:.4f}')\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    print(\"Model:\", model)\n",
    "    return model, t_list_loss, t_list_acc, v_list_loss, v_list_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Número de épocas\n",
    "epochs = 25\n",
    "\n",
    "# Entrenamiento del modelo\n",
    "model_trained, t_list_loss, t_list_acc, v_list_loss, v_list_acc = train(\n",
    "    base_model,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    epochs,\n",
    "    resume_train=False,\n",
    "    PATH='model_b5.pt'  # Cambia esta ruta según tu necesidad\n",
    ")\n",
    "\n",
    "# Guardar el modelo y el estado del optimizador\n",
    "torch.save({\n",
    "    'model_state_dict': model_trained.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    't_list_loss': t_list_loss,\n",
    "    't_list_acc': t_list_acc,\n",
    "    'v_list_loss': v_list_loss,\n",
    "    'v_list_acc': v_list_acc,\n",
    "}, 'model_b5_1.pt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cov_2_np(the_list):\n",
    "  new_list = []\n",
    "  for i in the_list:\n",
    "    new_list.append(i)\n",
    "  return new_list\n",
    "t_list_acc = cov_2_np(t_list_acc)\n",
    "v_list_acc = cov_2_np(v_list_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_list_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_impact(impact_value):\n",
    "    \"\"\"Evalúa si el impacto ambiental es bueno, moderado o malo.\"\"\"\n",
    "    if impact_value < 2000:\n",
    "        return \"Bajo (Bueno)\"\n",
    "    elif 2000 <= impact_value <= 6000:\n",
    "        return \"Moderado (Aceptable)\"\n",
    "    else:\n",
    "        return \"Alto (Necesita Mejorar)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visualización de las métricas incluyendo el impacto ambiental por época\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "\n",
    "# Gráfico de Precisión\n",
    "plt.subplot(3, 2, 1)\n",
    "plt.plot(epoch_precision, label='Precision', marker='o', color='blue')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision per Epoch')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# Gráfico de Recall\n",
    "plt.subplot(3, 2, 2)\n",
    "plt.plot(epoch_recall, label='Recall', marker='o', color='orange')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Recall')\n",
    "plt.title('Recall per Epoch')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# Gráfico de F1 Score\n",
    "plt.subplot(3, 2, 3)\n",
    "plt.plot(epoch_f1, label='F1 Score', marker='o', color='red')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('F1 Score per Epoch')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# Gráfico de Accuracy\n",
    "plt.subplot(3, 2, 4)\n",
    "plt.plot(epoch_accuracy, label='Accuracy', marker='o', color='green')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy per Epoch')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# Gráfico de Impacto Ambiental\n",
    "plt.subplot(3, 2, 5)\n",
    "plt.plot(epoch_impact, label='Impacto Ambiental', marker='o', color='purple')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Impacto Ambiental')\n",
    "plt.title('Impacto Ambiental Total por Época')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "# Ajustar los espacios entre gráficos para evitar solapamientos\n",
    "plt.tight_layout(pad=3.0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model):\n",
    "    start = time.time()\n",
    "    y_ = []\n",
    "    y = []\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        test_acc = 0.0\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs['image']\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            y = np.concatenate((y, labels.cpu().detach().numpy()))\n",
    "            y_ = np.concatenate((y_, preds.cpu().detach().numpy()))\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            test_loss += loss.item() * inputs.size(0)\n",
    "            test_acc += torch.sum(preds == labels)\n",
    "\n",
    "        test_loss = test_loss / len(test_dataset)\n",
    "        test_acc = test_acc.double() / len(test_dataset)\n",
    "\n",
    "    # Calcular métricas de rendimiento\n",
    "    accuracy = accuracy_score(y, y_)\n",
    "    precision = precision_score(y, y_, average='weighted', zero_division=0)\n",
    "    recall = recall_score(y, y_, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(y, y_, average='weighted', zero_division=0)\n",
    "\n",
    "    # Calcular el Impacto Ambiental Total\n",
    "    total_impact = 0\n",
    "    for true, pred in zip(y, y_):\n",
    "        if true != pred:  # Solo calcular el impacto si hay un error de clasificación\n",
    "            total_impact += impact_matrix.get((true, pred), 0)\n",
    "\n",
    "    impact_evaluation = evaluate_impact(total_impact)  # Evaluar el impacto ambiental total\n",
    "\n",
    "\n",
    "    total_time = time.time() - start\n",
    "    print(f'Testing complete in {total_time // 60:.0f}m {total_time % 60:.0f}s')\n",
    "    return y, y_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_label, predict_label = test(model_trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular la matriz de confusión\n",
    "confusion_matrix = confusion_matrix(true_label, predict_label)\n",
    "cm_display = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix)\n",
    "\n",
    "# Mostrar la matriz de confusión\n",
    "plt.subplots_adjust(bottom=0.2)\n",
    "cm_display.plot(cmap='Blues')\n",
    "plt.show()\n",
    "\n",
    "# Calcular métricas de rendimiento\n",
    "accuracy = accuracy_score(true_label, predict_label)\n",
    "precision = precision_score(true_label, predict_label, average='weighted', zero_division=0)\n",
    "recall = recall_score(true_label, predict_label, average='weighted', zero_division=0)\n",
    "f1 = f1_score(true_label, predict_label, average='weighted', zero_division=0)\n",
    "\n",
    "# Calcular el Impacto Ambiental Total y el número de errores\n",
    "total_impact = 0\n",
    "error_count = 0  # Contador de errores de clasificación\n",
    "for true, pred in zip(true_label, predict_label):\n",
    "    if true != pred:  # Solo calcular el impacto si hay un error de clasificación\n",
    "        total_impact += impact_matrix.get((true, pred), 0)\n",
    "        error_count += 1\n",
    "\n",
    "# Calcular el impacto medio si hay errores\n",
    "average_impact = total_impact / error_count if error_count > 0 else 0\n",
    "\n",
    "# Evaluar el impacto total\n",
    "impact_evaluation = evaluate_impact(total_impact)\n",
    "\n",
    "# Mostrar métricas y el impacto ambiental total\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1 Score: {f1:.4f}')\n",
    "print(f'Impacto Ambiental Total: {total_impact} - Evaluación: {impact_evaluation}')\n",
    "print(f'Impacto Ambiental Medio por Error: {average_impact:.2f}')\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calcular métricas por clase\n",
    "class_precisions = precision_score(true_label, predict_label, average=None, zero_division=0)\n",
    "class_recalls = recall_score(true_label, predict_label, average=None, zero_division=0)\n",
    "class_f1_scores = f1_score(true_label, predict_label, average=None, zero_division=0)\n",
    "\n",
    "# Calcular el número de imágenes por clase\n",
    "unique_classes, counts = np.unique(true_label, return_counts=True)\n",
    "\n",
    "# Calcular la precisión por clase utilizando la métrica accuracy de sklearn\n",
    "class_accuracies = [accuracy_score(true_label == i, predict_label == i) for i in unique_classes]\n",
    "\n",
    "# Calcular el impacto ambiental total por clase\n",
    "impact_per_class = []\n",
    "for i in unique_classes:\n",
    "    impact = 0\n",
    "    for true, pred in zip(true_label, predict_label):\n",
    "        if true == i and true != pred:  # Solo calcular el impacto si es de esa clase y es un error\n",
    "            impact += impact_matrix.get((true, pred), 0)\n",
    "    impact_per_class.append(impact)\n",
    "\n",
    "# Crear un DataFrame con todas las métricas\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Clases': [name for name, idx in sorted(class_dict.items(), key=lambda item: item[1])],\n",
    "    'Total Imágenes': counts,\n",
    "    'Precision': class_precisions,\n",
    "    'Accuracy': class_accuracies,\n",
    "    'Recall': class_recalls,\n",
    "    'F1 Score': class_f1_scores,\n",
    "    'Impacto Ambiental': impact_per_class\n",
    "})\n",
    "\n",
    "# Calcular el promedio de las métricas y la suma del total de imágenes\n",
    "averages = metrics_df[['Precision', 'Accuracy', 'Recall', 'F1 Score', 'Impacto Ambiental']].mean()\n",
    "total_images = metrics_df['Total Imágenes'].sum()\n",
    "\n",
    "# Crear una fila de promedios y añadirla al DataFrame\n",
    "totals = pd.Series({\n",
    "    'Clases': 'Total',\n",
    "    'Total Imágenes': total_images,\n",
    "    **averages\n",
    "})\n",
    "\n",
    "# Añadir la fila de promedios al DataFrame\n",
    "metrics_df = pd.concat([metrics_df, totals.to_frame().T], ignore_index=True)\n",
    "\n",
    "# Mostrar la tabla\n",
    "metrics_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
